
data:
  train_src: data/train_input_char.txt
  train_tgt: data/train_output_char.txt
  valid_src: data/dev_input_char.txt
  valid_tgt: data/dev_output_char.txt

# Prefix for preprocessed dataset (used by OpenNMT)
save_data: data/opennmt_char

# Vocabulary files (generated by onmt_build_vocab)
src_vocab: data/src_vocab.txt
tgt_vocab: data/tgt_vocab.txt
share_vocab: false  # true if you want the same vocab for src and tgt

# -----------------------------
# Model parameters
# -----------------------------
model_type: seq2seq
rnn_size: 512
word_vec_size: 256
encoder_type: brnn
decoder_type: rnn
layers: 2
dropout: 0.3

# -----------------------------
# Training parameters
# -----------------------------
batch_size: 64
optim: adam
learning_rate: 0.001
train_steps: 20000
valid_steps: 1000
save_checkpoint_steps: 2000
